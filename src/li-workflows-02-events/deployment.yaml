name: EventdrivenFlow

control-plane:
  port: 8000

default-service: agent_workflow

services:
  agent_workflow:
    name: Eventdriven AI
    # We tell Llama Deploy where to look for our workflow
    source:
      # In this case, we instruct Llama Deploy to look in the local filesystem
      type: local
      # The path in the local filesystem where to look. This assumes there's an src folder in the
      # current working directory containing the file app.py
      name: .
    # This assumes the file app.py contains a variable called `agent` containing our workflow instance
    path: app:agent_workflow